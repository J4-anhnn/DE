#!/usr/bin/env_python3
"""
BigQuery_Loader_Module

Vai_trò:_Module_này_chịu_trách_nhiệm_tải_dữ_liệu_vào_BigQuery_từ_các_nguồn_khác_nhau.
Nó_KHÔNG_tạo_dataset_hoặc_bảng_(việc_này_được_thực_hiện_bởi_Terraform).
Nó_chỉ_đảm_bảo_dữ_liệu_được_tải_vào_các_bảng_đã_tồn_tại.
"""
import_os
import_time
import_json
import_sys
from_typing_import_List,_Dict,_Any,_Optional,_Union
from_datetime_import_datetime
from_google.cloud_import_bigquery
from_google.cloud_import_storage
import_pandas_as_pd

#_Thêm_đường_dẫn_gốc_của_dự_án_vào_sys.path_để_import_config
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),_'..')))
from_config.settings_import_(
____GCP_PROJECT_ID,_BIGQUERY_DATASET,_BIGQUERY_TABLES,
____GCS_RAW_BUCKET,_GCS_PROCESSED_BUCKET,_GCS_TEMP_BUCKET,
____get_schema_path,_get_bigquery_table_id,_setup_logging
)

logger_=_setup_logging(__name__)

class_BigQueryLoader:
____"""Lớp_quản_lý_việc_tải_dữ_liệu_vào_BigQuery"""
____
____def___init__(self):
________"""Khởi_tạo_BigQuery_client_và_cấu_hình"""
________self.bq_client_=_bigquery.Client()
________self.storage_client_=_storage.Client()
________
________#_Đọc_cấu_hình
________self.project_id_=_GCP_PROJECT_ID
________self.dataset_id_=_BIGQUERY_DATASET
________self.tables_=_BIGQUERY_TABLES
________self.raw_bucket_=_GCS_RAW_BUCKET
________self.processed_bucket_=_GCS_PROCESSED_BUCKET
________self.temp_bucket_=_GCS_TEMP_BUCKET
________
________#_Kiểm_tra_tài_nguyên_BigQuery_đã_tồn_tại
________self._check_resources()
________
________logger.info(f"BigQuery_Loader_initialized_for_project_{self.project_id}")
____
____def__check_resources(self):
________"""Kiểm_tra_dataset_và_các_bảng_đã_tồn_tại_(được_tạo_bởi_Terraform)"""
________try:
____________dataset_ref_=_self.bq_client.dataset(self.dataset_id)
____________self.bq_client.get_dataset(dataset_ref)
____________logger.info(f"Dataset_{self.dataset_id}_exists")
____________
____________#_Kiểm_tra_các_bảng_cần_thiết
____________for_table_key,_table_id_in_self.tables.items():
________________table_ref_=_dataset_ref.table(table_id)
________________try:
____________________self.bq_client.get_table(table_ref)
____________________logger.info(f"Table_{table_id}_exists")
________________except_Exception_as_e:
____________________logger.warning(f"Table_{table_id}_does_not_exist:_{str(e)}")
________except_Exception_as_e:
____________logger.error(f"Error_checking_BigQuery_resources:_{str(e)}")
____________raise
____
____def_load_from_gcs(
________self,_
________source_bucket:_str,_
________source_path:_str,_
________table_key:_str,_
________write_disposition:_str_=_'WRITE_APPEND'
____)_->_int:
________"""
________Tải_dữ_liệu_từ_GCS_vào_BigQuery
________
________Args:
____________source_bucket:_Tên_bucket_GCS_chứa_dữ_liệu_nguồn
____________source_path:_Đường_dẫn_đến_file/thư_mục_trong_bucket
____________table_key:_Khóa_của_bảng_trong_BIGQUERY_TABLES
____________write_disposition:_Cách_xử_lý_dữ_liệu_hiện_có_(WRITE_APPEND,_WRITE_TRUNCATE,_WRITE_EMPTY)
____________
________Returns:
____________Số_hàng_đã_tải
________"""
________job_config_=_bigquery.LoadJobConfig(
____________source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
____________write_disposition=write_disposition,
________)
________
________#_Lấy_schema_từ_file_nếu_có
________table_id_=_self.tables.get(table_key.upper())
________if_not_table_id:
____________raise_ValueError(f"Unknown_table_key:_{table_key}")
________
________schema_path_=_get_schema_path(table_id)
________if_os.path.exists(schema_path):
____________with_open(schema_path,_'r')_as_f:
________________schema_json_=_json.load(f)
____________job_config.schema_=_[
________________bigquery.SchemaField(
____________________field['name'],
____________________field['type'],
____________________mode=field.get('mode',_'NULLABLE'),
____________________description=field.get('description',_'')
________________)
________________for_field_in_schema_json
____________]
________
________uri_=_f"gs://{source_bucket}/{source_path}"
________table_ref_=_f"{self.project_id}.{self.dataset_id}.{table_id}"
________
________logger.info(f"Loading_data_from_{uri}_to_{table_ref}")
________
________load_job_=_self.bq_client.load_table_from_uri(
____________uri,_table_ref,_job_config=job_config
________)
________
________try:
____________load_job.result()__#_Waits_for_the_job_to_complete
____________logger.info(f"Loaded_{load_job.output_rows}_rows_to_{table_id}")
____________return_load_job.output_rows
________except_Exception_as_e:
____________logger.error(f"Error_loading_data_to_BigQuery:_{str(e)}")
____________raise
____
____def_load_from_dataframe(
________self,_
________df:_pd.DataFrame,_
________table_key:_str,_
________write_disposition:_str_=_'WRITE_APPEND'
____)_->_int:
________"""
________Tải_dữ_liệu_từ_DataFrame_vào_BigQuery
________
________Args:
____________df:_DataFrame_chứa_dữ_liệu
____________table_key:_Khóa_của_bảng_trong_BIGQUERY_TABLES
____________write_disposition:_Cách_xử_lý_dữ_liệu_hiện_có_(WRITE_APPEND,_WRITE_TRUNCATE,_WRITE_EMPTY)
____________
________Returns:
____________Số_hàng_đã_tải
________"""
________table_id_=_self.tables.get(table_key.upper())
________if_not_table_id:
____________raise_ValueError(f"Unknown_table_key:_{table_key}")
________
________table_ref_=_f"{self.project_id}.{self.dataset_id}.{table_id}"
________
________job_config_=_bigquery.LoadJobConfig(
____________write_disposition=write_disposition,
________)
________
________#_Lấy_schema_từ_file_nếu_có
________schema_path_=_get_schema_path(table_id)
________if_os.path.exists(schema_path):
____________with_open(schema_path,_'r')_as_f:
________________schema_json_=_json.load(f)
____________job_config.schema_=_[
________________bigquery.SchemaField(
____________________field['name'],
____________________field['type'],
____________________mode=field.get('mode',_'NULLABLE'),
____________________description=field.get('description',_'')
________________)
________________for_field_in_schema_json
____________]
________
________logger.info(f"Loading_DataFrame_with_{len(df)}_rows_to_{table_ref}")
________
________try:
____________job_=_self.bq_client.load_table_from_dataframe(
________________df,_table_ref,_job_config=job_config
____________)
____________job.result()__#_Waits_for_the_job_to_complete
____________logger.info(f"Loaded_{len(df)}_rows_to_{table_id}")
____________return_len(df)
________except_Exception_as_e:
____________logger.error(f"Error_loading_DataFrame_to_BigQuery:_{str(e)}")
____________raise
____
____def_load_from_json(
________self,_
________json_data:_List[Dict[str,_Any]],_
________table_key:_str,_
________write_disposition:_str_=_'WRITE_APPEND'
____)_->_int:
________"""
________Tải_dữ_liệu_từ_JSON_vào_BigQuery
________
________Args:
____________json_data:_Danh_sách_các_dict_chứa_dữ_liệu
____________table_key:_Khóa_của_bảng_trong_BIGQUERY_TABLES
____________write_disposition:_Cách_xử_lý_dữ_liệu_hiện_có_(WRITE_APPEND,_WRITE_TRUNCATE,_WRITE_EMPTY)
____________
________Returns:
____________Số_hàng_đã_tải
________"""
________table_id_=_self.tables.get(table_key.upper())
________if_not_table_id:
____________raise_ValueError(f"Unknown_table_key:_{table_key}")
________
________table_ref_=_f"{self.project_id}.{self.dataset_id}.{table_id}"
________
________logger.info(f"Loading_JSON_data_with_{len(json_data)}_rows_to_{table_ref}")
________
________if_write_disposition_==_'WRITE_TRUNCATE':
____________#_Truncate_table_first
____________query_=_f"TRUNCATE_TABLE_`{table_ref}`"
____________self.bq_client.query(query).result()
____________logger.info(f"Truncated_table_{table_id}")
________
________try:
____________errors_=_self.bq_client.insert_rows_json(table_ref,_json_data)
____________if_errors_==_[]:
________________logger.info(f"Loaded_{len(json_data)}_rows_to_{table_id}")
________________return_len(json_data)
____________else:
________________logger.error(f"Errors_inserting_rows:_{errors}")
________________raise_Exception(f"Errors_inserting_rows:_{errors}")
________except_Exception_as_e:
____________logger.error(f"Error_loading_JSON_to_BigQuery:_{str(e)}")
____________raise
____
____def_execute_query(self,_query:_str)_->_bigquery.table.RowIterator:
________"""
________Thực_thi_truy_vấn_SQL_trên_BigQuery
________
________Args:
____________query:_Truy_vấn_SQL
____________
________Returns:
____________Kết_quả_truy_vấn
________"""
________logger.info(f"Executing_query:_{query}")
________
________try:
____________query_job_=_self.bq_client.query(query)
____________results_=_query_job.result()
____________logger.info(f"Query_executed_successfully")
____________return_results
________except_Exception_as_e:
____________logger.error(f"Error_executing_query:_{str(e)}")
____________raise
____
____def_load_batch_data(
________self,_
________date_str:_Optional[str]_=_None,_
________source_bucket:_Optional[str]_=_None
____)_->_int:
________"""
________Tải_dữ_liệu_batch_từ_GCS_vào_BigQuery
________
________Args:
____________date_str:_Ngày_dữ_liệu_(YYYY-MM-DD),_mặc_định_là_hôm_nay
____________source_bucket:_Tên_bucket_GCS,_mặc_định_là_GCS_PROCESSED_BUCKET
____________
________Returns:
____________Số_hàng_đã_tải
________"""
________if_date_str_is_None:
____________date_str_=_datetime.now().strftime("%Y-%m-%d")
________
________source_bucket_=_source_bucket_or_self.processed_bucket
________source_path_=_f"batch/{date_str}/*.json"
________
________return_self.load_from_gcs(source_bucket,_source_path,_'BATCH')
____
____def_load_streaming_data(
________self,_
________date_str:_Optional[str]_=_None,_
________source_bucket:_Optional[str]_=_None
____)_->_int:
________"""
________Tải_dữ_liệu_streaming_từ_GCS_vào_BigQuery
________
________Args:
____________date_str:_Ngày_dữ_liệu_(YYYY-MM-DD),_mặc_định_là_hôm_nay
____________source_bucket:_Tên_bucket_GCS,_mặc_định_là_GCS_PROCESSED_BUCKET
____________
________Returns:
____________Số_hàng_đã_tải
________"""
________if_date_str_is_None:
____________date_str_=_datetime.now().strftime("%Y-%m-%d")
________
________source_bucket_=_source_bucket_or_self.processed_bucket
________source_path_=_f"streaming/{date_str}/*.json"
________
________return_self.load_from_gcs(source_bucket,_source_path,_'STREAMING')
____
____def_generate_analytics(self,_date_str:_Optional[str]_=_None)_->_int:
________"""
________Tạo_dữ_liệu_phân_tích_từ_dữ_liệu_thô_và_tải_vào_bảng_analytics
________
________Args:
____________date_str:_Ngày_dữ_liệu_(YYYY-MM-DD),_mặc_định_là_hôm_nay
____________
________Returns:
____________Số_hàng_đã_tải
________"""
________if_date_str_is_None:
____________date_str_=_datetime.now().strftime("%Y-%m-%d")
________
________#_Kiểm_tra_xem_bảng_nguồn_có_tồn_tại_không
________source_table_=_None
________if_self.tables.get('BATCH')_in_self._check_resources():
____________source_table_=_self.tables.get('BATCH')
________elif_self.tables.get('STREAMING')_in_self._check_resources():
____________source_table_=_self.tables.get('STREAMING')
________else:
____________raise_ValueError("No_source_table_available_for_analytics")
________
________#_Tạo_truy_vấn_để_tính_toán_phân_tích
________metrics_=_["temperature",_"humidity",_"pressure",_"wind_speed"]
________date_field_=_"processing_date"_if_source_table_==_self.tables.get('BATCH')_else_"DATE(timestamp)"
________
________for_metric_in_metrics:
____________query_=_f"""
____________INSERT_INTO_`{self.project_id}.{self.dataset_id}.{self.tables.get('ANALYTICS')}`
____________(city_name,_date,_metric_type,_avg_value,_min_value,_max_value,_std_dev,_processing_time)
____________SELECT
________________city_name,
________________{date_field}_as_date,
________________'{metric}'_as_metric_type,
________________AVG({metric})_as_avg_value,
________________MIN({metric})_as_min_value,
________________MAX({metric})_as_max_value,
________________STDDEV({metric})_as_std_dev,
________________CURRENT_TIMESTAMP()_as_processing_time
____________FROM
________________`{self.project_id}.{self.dataset_id}.{source_table}`
____________WHERE
________________{date_field}_=_'{date_str}'
____________GROUP_BY
________________city_name,_{date_field}
____________"""
____________
____________logger.info(f"Generating_analytics_for_{metric}_on_{date_str}")
____________self.execute_query(query)
________
________#_Đếm_số_hàng_đã_tạo
________count_query_=_f"""
________SELECT_COUNT(*)_as_count
________FROM_`{self.project_id}.{self.dataset_id}.{self.tables.get('ANALYTICS')}`
________WHERE_date_=_'{date_str}'
________"""
________
________result_=_list(self.execute_query(count_query))
________row_count_=_result[0]['count']_if_result_else_0
________
________logger.info(f"Generated_{row_count}_analytics_rows_for_{date_str}")
________return_row_count

def_main():
____"""Hàm_chính_để_chạy_loader"""
____import_argparse
____
____parser_=_argparse.ArgumentParser(description='BigQuery_Loader')
____parser.add_argument('--mode',_choices=['batch',_'streaming',_'analytics'],_default='batch',
______________________help='Mode_to_run_the_loader_in')
____parser.add_argument('--date',_help='Date_to_process_(YYYY-MM-DD)')
____parser.add_argument('--source-bucket',_help='Source_GCS_bucket')
____args_=_parser.parse_args()
____
____logger.info("BigQuery_loader_started")
____
____try:
________loader_=_BigQueryLoader()
________
________if_args.mode_==_'batch':
____________rows_=_loader.load_batch_data(args.date,_args.source_bucket)
____________logger.info(f"Loaded_{rows}_batch_rows")
________elif_args.mode_==_'streaming':
____________rows_=_loader.load_streaming_data(args.date,_args.source_bucket)
____________logger.info(f"Loaded_{rows}_streaming_rows")
________elif_args.mode_==_'analytics':
____________rows_=_loader.generate_analytics(args.date)
____________logger.info(f"Generated_{rows}_analytics_rows")
________
____except_KeyboardInterrupt:
________logger.info("Loader_stopped_by_user")
____except_Exception_as_e:
________logger.error(f"Error_in_loader:_{str(e)}")
________import_sys
________sys.exit(1)

if___name___==_"__main__":
____main()

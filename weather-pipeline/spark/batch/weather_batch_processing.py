#!/usr/bin/env_python3
"""
Spark_batch_job_to_process_weather_data_from_GCS_and_load_into_BigQuery
"""
from_pyspark.sql_import_SparkSession
from_pyspark.sql.functions_import_col,_from_json,_to_timestamp,_expr,_current_timestamp,_lit
from_pyspark.sql.types_import_StructType,_StructField,_StringType,_DoubleType,_TimestampType
import_argparse
from_datetime_import_datetime
import_os
import_sys
import_json

#_Thêm_đường_dẫn_gốc_của_dự_án_vào_sys.path_để_import_config
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__),_'../..')))
try:
____from_config.settings_import_GCP_PROJECT_ID,_BIGQUERY_DATASET,_BIGQUERY_TABLES,_get_schema_path
except_ImportError:
____#_Fallback_nếu_không_import_được_config
____GCP_PROJECT_ID_=_os.environ.get('GCP_PROJECT_ID')
____BIGQUERY_DATASET_=_os.environ.get('BIGQUERY_DATASET',_'weather_data')
____BIGQUERY_TABLES_=_{
________'BATCH':_'weather_batch'
____}
____
____def_get_schema_path(table_name):
________return_None

def_create_spark_session():
____"""Create_and_configure_Spark_session"""
____return_(SparkSession.builder
____________.appName("WeatherBatchProcessing")
____________.config("spark.jars",_"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar")
____________.getOrCreate())

def_process_weather_data(spark,_input_path,_output_table,_processing_date):
____"""Process_weather_data_from_GCS_and_load_into_BigQuery"""
____#_Define_schema_for_raw_weather_data
____schema_=_StructType([
________StructField("coord",_StructType([
____________StructField("lon",_DoubleType()),
____________StructField("lat",_DoubleType())
________])),
________StructField("weather",_StringType()),
________StructField("base",_StringType()),
________StructField("main",_StructType([
____________StructField("temp",_DoubleType()),
____________StructField("feels_like",_DoubleType()),
____________StructField("temp_min",_DoubleType()),
____________StructField("temp_max",_DoubleType()),
____________StructField("pressure",_DoubleType()),
____________StructField("humidity",_DoubleType())
________])),
________StructField("visibility",_DoubleType()),
________StructField("wind",_StructType([
____________StructField("speed",_DoubleType()),
____________StructField("deg",_DoubleType())
________])),
________StructField("clouds",_StructType([
____________StructField("all",_DoubleType())
________])),
________StructField("dt",_DoubleType()),
________StructField("sys",_StructType([
____________StructField("type",_DoubleType()),
____________StructField("id",_DoubleType()),
____________StructField("country",_StringType()),
____________StructField("sunrise",_DoubleType()),
____________StructField("sunset",_DoubleType())
________])),
________StructField("timezone",_DoubleType()),
________StructField("id",_DoubleType()),
________StructField("name",_StringType()),
________StructField("cod",_DoubleType()),
________StructField("city_name",_StringType()),
________StructField("collected_at",_StringType()),
________StructField("airflow_execution_date",_StringType()),
________StructField("airflow_dag_run_id",_StringType())
____])
____
____#_Read_JSON_files_from_GCS
____df_=_spark.read.schema(schema).json(input_path)
____
____#_Extract_and_transform_data
____processed_df_=_df.select(
________col("city_name"),
________col("name").alias("city_original_name"),
________col("coord.lon").alias("longitude"),
________col("coord.lat").alias("latitude"),
________col("main.temp").alias("temperature"),
________col("main.feels_like").alias("feels_like"),
________col("main.humidity").alias("humidity"),
________col("main.pressure").alias("pressure"),
________col("wind.speed").alias("wind_speed"),
________col("wind.deg").alias("wind_direction"),
________col("clouds.all").alias("cloudiness"),
________expr("explode(from_json(weather,_'array<struct<main:string,description:string>>'))")
____________.alias("weather_data"),
________to_timestamp(col("dt")).alias("measurement_time"),
________to_timestamp(col("sys.sunrise")).alias("sunrise"),
________to_timestamp(col("sys.sunset")).alias("sunset"),
________current_timestamp().alias("processing_time"),
________to_timestamp(lit(processing_date)).cast("date").alias("processing_date")
____)
____
____#_Extract_weather_condition_and_description
____final_df_=_processed_df.select(
________col("city_name"),
________col("city_original_name"),
________col("longitude"),
________col("latitude"),
________col("temperature"),
________col("feels_like"),
________col("humidity"),
________col("pressure"),
________col("wind_speed"),
________col("wind_direction"),
________col("cloudiness"),
________col("weather_data.main").alias("weather_condition"),
________col("weather_data.description").alias("weather_description"),
________col("measurement_time"),
________col("sunrise"),
________col("sunset"),
________col("processing_time"),
________col("processing_date"),
________expr("(unix_timestamp(sunset)_-_unix_timestamp(sunrise))_/_3600").alias("day_length_hours")
____)
____
____#_Write_to_BigQuery
____final_df.write_\
________.format("bigquery")_\
________.option("table",_output_table)_\
________.option("temporaryGcsBucket",_"weather-temp-bucket")_\
________.mode("append")_\
________.save()
____
____return_final_df.count()

def_main():
____"""Main_entry_point"""
____parser_=_argparse.ArgumentParser(description='Process_weather_data_from_GCS_to_BigQuery')
____parser.add_argument('--input_path',_required=True,_help='GCS_path_to_input_data')
____parser.add_argument('--output_table',_help='BigQuery_output_table_(project.dataset.table)')
____parser.add_argument('--processing_date',_required=True,_help='Processing_date_(YYYY-MM-DD)')
____
____args_=_parser.parse_args()
____
____#_Nếu_không_có_output_table,_sử_dụng_cấu_hình_mặc_định
____output_table_=_args.output_table
____if_not_output_table:
________output_table_=_f"{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{BIGQUERY_TABLES['BATCH']}"
____
____spark_=_create_spark_session()
____
____try:
________record_count_=_process_weather_data(
____________spark,_
____________args.input_path,_
____________output_table,_
____________args.processing_date
________)
________print(f"Successfully_processed_{record_count}_weather_records")
____except_Exception_as_e:
________print(f"Error_processing_weather_data:_{str(e)}")
________raise
____finally:
________spark.stop()

if___name___==_"__main__":
____main()

ğŸš€ Tá»•ng quan
Project nÃ y triá»ƒn khai má»™t pipeline Ä‘Æ¡n giáº£n nhÆ°ng Ä‘áº§y Ä‘á»§ Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u, sá»­ dá»¥ng:

Kestra: Quáº£n lÃ½ vÃ  Ä‘iá»u phá»‘i workflow
Kafka: Message broker Ä‘á»ƒ truyá»n dá»¯ liá»‡u
Spark: Xá»­ lÃ½ dá»¯ liá»‡u phÃ¢n tÃ¡n
ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng
[Kestra Flow] --> [Kafka Producer] --> [Kafka Topic] --> [Spark Consumer] --> [Storage]
ğŸ“‹ Chi tiáº¿t cÃ¡c thÃ nh pháº§n
1ï¸âƒ£ Kestra Flow
Kestra Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘iá»u phá»‘i toÃ n bá»™ luá»“ng dá»¯ liá»‡u vá»›i hai tÃ¡c vá»¥ chÃ­nh:

Flow: â€œsimple-data-pipelineâ€
TÃ¡c vá»¥ 1: ingest_data

KÃ­ch hoáº¡t script Python kafka_producer.py
Táº¡o vÃ  gá»­i 20 báº£n ghi dá»¯ liá»‡u giáº£ láº­p vÃ o Kafka topic â€œinput-dataâ€
Má»—i báº£n ghi chá»©a id, value ngáº«u nhiÃªn, vÃ  timestamp
TÃ¡c vá»¥ 2: process_data

Phá»¥ thuá»™c vÃ o hoÃ n thÃ nh cá»§a tÃ¡c vá»¥ â€œingest_dataâ€
KÃ­ch hoáº¡t Spark job qua spark-submit
Cháº¡y script spark_consumer.py Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u tá»« Kafka
CÃ¡ch Kestra Ä‘iá»u phá»‘i:
Theo dÃµi tráº¡ng thÃ¡i: GiÃ¡m sÃ¡t vÃ  bÃ¡o cÃ¡o tiáº¿n trÃ¬nh cá»§a tá»«ng tÃ¡c vá»¥
Xá»­ lÃ½ phá»¥ thuá»™c: Äáº£m báº£o thá»© tá»± cháº¡y Ä‘Ãºng cÃ¡c tÃ¡c vá»¥
Retry logic: Tá»± Ä‘á»™ng thá»­ láº¡i khi cÃ³ lá»—i
2ï¸âƒ£ Kafka
Topic: â€œinput-dataâ€
Partitions: 1 (Ä‘Æ¡n giáº£n cho demo)
Retention: Máº·c Ä‘á»‹nh
Message format: JSON vá»›i schema Ä‘Æ¡n giáº£n
{
"id": 1,
"value": 75,
"timestamp": 1715181600.123
}
Copy
3ï¸âƒ£ Spark
Spark xá»­ lÃ½ dá»¯ liá»‡u vá»›i cÃ¡c bÆ°á»›c sau:

Xá»­ lÃ½ chÃ­nh:
Äá»c dá»¯ liá»‡u tá»« Kafka:

Sá»­ dá»¥ng Structured Streaming
Äá»c tá»« topic â€œinput-dataâ€
Parse dá»¯ liá»‡u JSON thÃ nh DataFrame cÃ³ cáº¥u trÃºc
Biáº¿n Ä‘á»•i dá»¯ liá»‡u:

TrÃ­ch xuáº¥t cÃ¡c trÆ°á»ng tá»« JSON
ThÃªm cá»™t má»›i â€œprocessed_valueâ€ = value \* 2
Ãp dá»¥ng cÃ¡c transformation Ä‘Æ¡n giáº£n trÃªn dá»¯ liá»‡u gá»‘c
Xá»­ lÃ½ dá»¯ liá»‡u streaming:

Xá»­ lÃ½ theo micro-batch
Má»—i batch Ä‘Æ°á»£c xá»­ lÃ½ vÃ  ghi ra ngoÃ i
LÆ°u trá»¯ káº¿t quáº£:

Ghi dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ ra Ä‘á»‹nh dáº¡ng CSV
LÆ°u vÃ o Ä‘Æ°á»ng dáº«n Ä‘á»‹nh trÆ°á»›c vá»›i checkpoint Ä‘á»ƒ Ä‘áº£m báº£o exactly-once processing
ğŸ”§ Cáº¥u hÃ¬nh vÃ  triá»ƒn khai
Docker Compose
File docker-compose.yml cung cáº¥p cÃ¡c service cáº§n thiáº¿t:

Zookeeper: Quáº£n lÃ½ cluster Kafka
Kafka: Message broker
Kestra: Orchestration engine
Spark: Processing engine
Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng

# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ service

docker-compose up -d

# Kiá»ƒm tra tráº¡ng thÃ¡i

docker-compose ps
Copy
Truy cáº­p UI
Kestra UI: http://localhost:8081
Spark UI: http://localhost:8090
ğŸ“Š Luá»“ng dá»¯ liá»‡u
Kestra khá»Ÿi cháº¡y producer script Ä‘á»ƒ táº¡o dá»¯ liá»‡u giáº£ láº­p
Dá»¯ liá»‡u Ä‘Æ°á»£c gá»­i vÃ o Kafka topic â€œinput-dataâ€
Kestra kÃ­ch hoáº¡t Spark job Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u
Spark Ä‘á»c dá»¯ liá»‡u tá»« Kafka, xá»­ lÃ½ vÃ  lÆ°u káº¿t quáº£
Káº¿t quáº£ cuá»‘i cÃ¹ng lÆ°u dÆ°á»›i dáº¡ng file CSV
ğŸ’» MÃ£ nguá»“n
Kafka Producer (kafka_producer.py)

# Táº¡o 20 báº£n ghi dá»¯ liá»‡u giáº£ láº­p vÃ  gá»­i vÃ o Kafka

# Má»—i báº£n ghi cÃ³ id, giÃ¡ trá»‹ ngáº«u nhiÃªn, vÃ  timestamp

Copy
Spark Consumer (spark_consumer.py)

# Káº¿t ná»‘i vá»›i Kafka vÃ  Ä‘á»c dá»¯ liá»‡u tá»« topic "input-data"

# Parse JSON thÃ nh DataFrame Spark cÃ³ cáº¥u trÃºc

# Táº¡o cá»™t má»›i "processed_value" = value \* 2

# LÆ°u káº¿t quáº£ ra Ä‘á»‹nh dáº¡ng CSV
